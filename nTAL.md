# TAL

Traitement automatique du langage, NLP

## Textométrie

Selon M. Tournier, la lexicometrie aussi appelée : logométrie, analyse automatique,

« **statistique linguistique** (Guiraud, 1959, 1960), **statistique lexicale** ou **linguistique quantitative** (Muller, 1964, 1967, 1973, 1979), **statistique textuelle** (Salem 1987, 1994), voire **analyse des données en linguistique** (Benzecri 1981), **la lexicométrie**  (Tournier 1975, Lafon 1984) n’est pas une théorie mais une méthodologie  d’étude du discours, qui se veut exhaustive, systématique est  automatisée. » Tournier M., 2002, p. 342-343.

Michel Pêcheux 1969, Analyse automatique du discours (AAD) à partir d’une approche syntaxique à la suite de Z. Harris. Mais son approche nécessitait une préparation importante du corpus. L’usage d’une codification thématique mobilisant déjà une analyse et un jugement (Achard 1991). —> Volonté de travailler sur la langue naturelle, sans préparation particulière du texte.

G. Herdan (1964) considère la linguistique statistique comme « une branche de la linguistique structurale, avec pour principale fonction la description statistique du fonctionnement (dans des corpus de textes) des unités définies par le linguiste aux différents niveaux de l’analyse linguistique (phonologique, lexical, phrastique) » cf. Lebart L., Salem A., 1994, p.16.

D. Mayaffre historien discours politiques

Mayaffre D., 2000, p.749. explique que ses connaissances linguistiques ne lui suffisent pas pour nourrir le débat épistémologique sur le lien entre Histoire et Linguistique dans l’analyse du discours

 ni à la méthode binomiale ni le modèle hypergéométrique en statistique lexicale

fait du mot un “objet réticulaire” pour Mayaffre

Traitement lexicographique qui consiste à traiter le texte sans lemmatisation (pour lemmatiser le vocabulaire d’un texte écrit en français, on ramène en général les formes verbales à l’infinitif, les substantifs au singulier, les adjectifs au masculin singulier, les formes élidées à la forme sans élision). Les formes apparaissent donc telles qu’elles ont été saisies : une forme au singulier et au pluriel comptant pour deux formes différentes, de même qu’un adjectif féminin ou masculin, ou encore un verbe sous ses différentes formes conjuguées.

Le corpus est distribué en plusieurs parties dans l’édition avant l’opération de segmentation. On y introduit différentes clés qui correspondent à des variables dont on dispose par ailleurs et qui peuvent permettent, par exemple, d’identifier des locuteurs puis de les regrouper par âge, sexe, niveau d’étude, etc.

Les traitements lexicométriques reposent sur une segmentation automatique du texte en occurrences de formes graphiques à partir de la définition d’un sous-ensemble de caractères délimiter. Les autres caractères étant considérés comme non délimiter. Chaque suite de caractères bornée à ses deux extrémités par des caractères délimiter est considérée comme une occurence. Et deux suites de caractères non délimiter indentifiques constituent deux occurrences d’une même forme. « La forme est un archétype correspondant à une ensemble d’occurrences identiques. L’ensemble des formes d’un texte constitue son vocabulaire. » (Leimdorfer et Salem 1995)

Cette segmentation en formes graphiques permet ensuite de considérer le texte comme une suite d’occurrences séparées entre elles par un ou plusieurs caractères délimiter. « On regroupe sous le terme de lexicométrie toute une série de méthodes qui permettent d’opérer, à partir d’une segmentation, des réorganisations formelles de la séquence textuelle et des analyses statistiques portant sur le vocabulaire. » (Leimdorfer et Salem 1995)

- méthodes documentaires qui opèrent une simple réorganisation de la surface textuelle
- méthodes qui opèrent, pour chaque texte pris isolément, des comptages et des calculs d’indices statistiques
- les méthodes statistiques contrastives qui produisent des résultats portant sur le vocabulaire de chacun des textes par rapport à l’ensemble des textes réunis dans un même corpus à des fins de comparaisons.

Logiciels qui fournissent après la segmentation série de documents permettant de mieux appréhender le vocabulaire du corpus

- L’*index alphabétique* permet de vérifier la saisie du texte, de rapprocher les utilisations du singulier et du pluriel d’un même substantif, les différentes flexions d’un verbe, etc.
- L’*index hiérarchique*, dans lesquels les formes sont classées par fréquence décroissante, permet d’examiner les formes les plus utilisées
- Les *concordances* permettent, pour chaque forme, de rassembler l’ensemble des contextes dans lesquels la forme apparaît
- Les *inventaires de segments répétés* permettent de repérer les séquences de formes qui apparaissent à plusieurs endroits du texte
- le *calcul de spécificités* permet de dégager les formes et les segments qui se trouvent être particulièrement employés (ou, au contraire, particulièrement sous-employés) dans chacune des parties du corpus.

[...]

L’outil lexicométrique s’avère, du point de vue de l’analyse de discours, d’un très grand intérêt, dans trois directions principales :

- par les données quantitatives fournies, les comparaisons et les vérifications qu’il permet ;
- comme outil de repérage de pistes de recherche, et comme premier bilan d’un corpus ;
- comme outil heuristique puissant, entraînant à des allers-retours fructueux entre le texte analysé et les données produites. Il incite à une définition plus fine des données et à des comparaisons vers d’autres corpus. Il oblige également à une réflexion sur le statut du « quantitatif » dans le discours à l’écrit et à l’oral.

D’un point de vue pratique, il est particulièrement utile si **plusieurs éléments se trouvent réunis** :

- si le corpus est relativement important, difficilement maîtrisable par une analyse fine de fragments ; mais des informations intéressantes se dégagent avec des corpus de quelques dizaines de pages seulement ;
- si la saisie sous traitement de texte peut être faite sans difficultés particulières, et de manière économique (en temps de travail notamment) ;
- si le corpus est suffisamment connu, déjà analysé pour que les indications statistiques données puissent prendre sens et orienter la recherche ; lorsque cette connaissance de l’« l’intérieur » n’existe pas, les données fournies par le calcul indiquent autant de pistes possibles, mais qu’il est difficile d’examiner exhaustivement. Par contre, lorsque le corpus a déjà été analysé en partie ou que l’on dispose de pistes de recherche identifiées, l’outil lexicométrique devient un « multiplicateur » de recherche remarquable, par les allers-retours continuels qu’il permet entre l’analyse de fragments, les données statistiques et les nouvelles demande de tri que l’on peut formuler.

(Leimdorfer, François et André Salem. 1995. « Usages de la lexicométrie en analyse de discours ». *Cahiers de Sciences humaines* 31 (1) : 131-143. décrivent ensuite l’exemple de thèses)



## Ressources pédagogiques

- Informatique et Linguistique de Jean Véronis (nous contacter pour y accéder)
- [Introduction au TALN et à l'ingénierie linguistique](http://www.lattice.cnrs.fr/sites/itellier/poly_info_ling/index.html) de Isabelle Tellier
- [Instruments et ressources électroniques pour le français](http://www.amazon.fr/Instruments-ressources-électroniques-pour-français/dp/2708011197) de Benoît Habert
- [*Python NLTK Demos for Natural Language Text Processing*](http://text-processing.com/demo/)
- [Machine Translation](https://mitpress.mit.edu/books/machine-translation-0), Thierry Poibeau, MIT Press, 2017
- [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf), Jacon Eisenstein, 2018
- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft), Dan Jurafsky and James H. Martin, 2018 
- [Foundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/), Chris Manning and Hinrich Schütze, 1999

## Expressions régulières

- [Regex Cross­word](http://www.regexcrossword.com/). *Welcome to the fantastic world of nerdy regex fun! Start playing by selecting one of the puzzle challenges below. There are a wide range of difficulties from beginner to expert.*
- [Regular Expression Test Page for Perl](http://www.regexplanet.com/advanced/perl/index.html)
- [Quick start regex for analysis](http://www.coppelia.io/quick-start-regex-for-analysts-part-i/)
- [Introduction aux expressions régulières de Perl 5 et PCRE](http://youtu.be/QTvi77m0pco?a) ([slides ici ](http://maddingue.free.fr/conferences/fpw-2014/regexp/))

http://perl.linguistes.free.fr

## Statistiques

- [La Statistique en clair](http://www.editions-ellipses.fr/product_info.php?manufacturers_id=358&products_id=7900) de François Grosjean, Jean-Yves Dommergues et Gilles Macagno
- [**Ludovic Lebart, André Salem** (1994) **Statistique Textuelle**](http://lexicometrica.univ-paris3.fr/livre/st94/st94-tdm.html), Dunod.
- [**Stat Trek**](http://stattrek.com/) Teach yourself statistics

## Linguistique générale

- Nouveau dictionnaire encyclopédique des sciences du langage, Ducrot Oswald, Schaeffer Jean-Marie, Seuil, 1995.
- Introduction à la linguistique, 3 volumes, Milicevic Jasmina, Mel'cuk Igor, Hermann, 2014.
- Un blog de linguistique "pour tout le monde" : https://bling.hypotheses.org/

## Lectures classiques

- Cours de linguistique générale, Ferdinand de Saussure, 1916, réédité chez Payot.
- Langage, Leonard Bloomfield, 1933, traduction française.
- Structures syntaxiques, Noam Chomsky, 1957, traduction française au Seuil.
- Eléments de syntaxe structurale, Lucien Tesnière, 1959, Klincksieck.
- Eléments de linguistique générale, André Martinet; 1970, Armand Colin.
- Problèmes de linguistique générale, Emile Benveniste, 1966, Gallimard.

## Signets

### Lexicometrica

ISSN 1773-0570

http://lexicometrica.univ-paris3.fr

La revue LEXICOMETRICA s'adresse aux chercheurs, aux étudiants, aux professionnels de la communication et de la fouille de données textuelles... intéressés par les travaux théoriques et pratiques menés dans les domaines suivants : Lexicométrie / statistique textuelle, linguistiques de corpus, extraction d'informations à partir de corpus de texte, acquisition de connaissances...

### Revue Corpus

https://journals.openedition.org/corpus/397

### Semen, revue de sémio-linguistique des textes et discours

https://journals.openedition.org/semen

### Revue Texto, Textes & Cultures

http://www.revue-texto.net/****

### Google Ngram Viewer (2013)

https://books.google.com/ngrams

### Atelier Cahier 2019

Exploiter les corpus d’auteur – Atelier de formation annuel du consortium Cahier – Poitiers – 18-20 juin 2019 – ouverture des inscriptions

https://cahier.hypotheses.org/4662

Vos formateurs pour 2019 seront : Dr.[Peter Stockes](http://www.peterstokes.org/cv/index.html) (École Pratique des Hautes Études), Directeur d’Etudes à l’EPHE [Raphaël Céré](http://igd.unil.ch/rcere/) (Université de Lausanne), Doctorant à l’Université de Lausanne Dr. [Aris Xanthos](https://www.unil.ch/sli/fr/home/menuinst/collaborateurs/xanthos-aris.html) (Université de Lausanne), Maître d’enseignement et de recherche à l’Université de Lausanne. 

https://github.com/pastokes/python_tei

http://textable.io/cahier/

Revue en ligne *Kairos*

http://kairos.technorhetoric.net/archive.html